{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_ip = \"localhost\"\n",
    "milvus_port = 19530\n",
    "db_name = \"wb_vanila_rag\"\n",
    "collection_name = \"news_article\"\n",
    "collection_kor_name = \"뉴스 기사 데이터\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d3e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: 컬렉션 생성 (create_collection)\n",
    "from pymilvus import (Collection, DataType, FieldSchema, CollectionSchema, \n",
    "                    connections, MilvusClient, Function, FunctionType)\n",
    "\n",
    "# Milvus 연결\n",
    "connections.connect(host=milvus_ip, port=milvus_port, db_name=db_name)\n",
    "milvus_client = MilvusClient(uri=f\"http://{milvus_ip}:{milvus_port}\", db_name=db_name)\n",
    "\n",
    "selected_embedding_model = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "milvus_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e125bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스키마 정의 (기존 create_collection과 동일)\n",
    "bm25_function = Function(\n",
    "        name=\"text_bm25_emb\",\n",
    "        input_field_names=[\"chunk\"],\n",
    "        output_field_names=[\"sparse_vector\"],\n",
    "        function_type=FunctionType.BM25,\n",
    "    )\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"ver\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"access_level\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"params\", dtype=DataType.JSON),\n",
    "    FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "    FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=milvus_dim),\n",
    "    FieldSchema(name=\"section\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"chunk\", dtype=DataType.VARCHAR, max_length=65535, enable_analyzer=True),\n",
    "    FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "]        \n",
    "    \n",
    "schema = CollectionSchema(fields, description=f\"{collection_kor_name}\")\n",
    "schema.add_function(bm25_function)\n",
    "\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# 인덱스 생성\n",
    "index_params = {\"metric_type\": \"IP\", \"index_type\": \"FLAT\", \"params\": {}}\n",
    "collection.create_index(field_name=\"dense_vector\", index_params=index_params)\n",
    "sparse_index_params = {\"metric_type\": \"BM25\", \"index_type\": \"SPARSE_INVERTED_INDEX\", \"params\": {}}\n",
    "\n",
    "collection.create_index(field_name=\"sparse_vector\", index_params=sparse_index_params)\n",
    "\n",
    "# 컬렉션을 메모리에 로드하여 사용 준비\n",
    "collection.load()\n",
    "connections.disconnect(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46990efb",
   "metadata": {},
   "source": [
    "md2json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8af028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "CHUNK_SIZE = 200\n",
    "CHUNK_OVERLAP = 20\n",
    "SEPARATOR = [\"\\n\\n\"]\n",
    "MD_FOLDER = \"/home/woobin.choi/research/mlops/rag/rsc/raws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e137962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=SEPARATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(filepath, filename, json_data):\n",
    "    directory_path = \"/\".join(filepath.split(\"/\")[:-1])\n",
    "    if \"raws\" in directory_path:\n",
    "        directory_path = directory_path.replace(\"raws\", \"processed\")\n",
    "    \n",
    "    # 디렉토리가 없으면 생성\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    json_filename = filename.replace(\".md\", \"_chunks.json\")\n",
    "    json_filepath = os.path.join(directory_path, json_filename)\n",
    "\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da649ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/woobin.choi/research/mlops/rag/rsc/raws'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5760ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_DATA_PATTERN = os.path.join(MD_FOLDER, \"**\", \"*.md\")\n",
    "md_paths = glob.glob(MD_DATA_PATTERN, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f3349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_paths = [os.path.abspath(p) for p in md_paths]\n",
    "filepaths = [md for md in md_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b84b70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b291c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/woobin.choi/research/mlops/rag/rsc/raws/19961.md'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ac551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Params(BaseModel):\n",
    "    chunk_size: int = Field(..., description=\"청크 사이즈\")\n",
    "    chunk_overlap: int = Field(..., description=\"청크 중첩 사이즈\")\n",
    "    length_function: str = Field(..., description=\"길이 함수, len | count_tokens\")\n",
    "    separator: List[Optional[str]] = Field(..., description=\"구분자, 예: ['\\\\n\\\\n', None]\")\n",
    "    splitter: str = Field(..., description=\"텍스트 분리기, RecursiveCharacterTextSplitter | MarkdownTextSplitter\")\n",
    "    parser: str = Field(..., description=\"PDF 파서 종류, PyMuPDFLoader | pymupdf4llm\")\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    title: str = Field(\"\", description=\"연구 제목\", example=\"초거대 AI를 활용한 RAG 시스템 구축\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    chunks: List[List[str]] = Field(..., description=\"청크 목록\")\n",
    "    section: str = Field(\"\", description=\"chunk가 속한 문서의 절절\")\n",
    "\n",
    "class DataSpec(BaseModel):\n",
    "    ver: str = Field(..., description=\"해당 데이터의 버전\")\n",
    "    access_level: int = Field(..., description=\"접근 권한 레벨, 1 | 2 | 3\")\n",
    "    params: Params\n",
    "    metadata: Metadata\n",
    "    data: Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e0698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(filepaths[0])\n",
    "\n",
    "docs = loader.load_and_split(text_splitter)\n",
    "print(len(docs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09814bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n* Patty와 남편 Tyler는 도시에서도 교외 같은 공동체 느낌을 만들고자 노력함\\n* 처음엔 쿠키를 만들어 돌리거나 저녁 식사에 초대할까 고민했지만, 결국 주말 아침에 집 밖에서 커피를 마시기로 결정함\\n* 비록 집에 현관 계단은 없었지만, 접이식 의자를 들고 나와 햇볕을 즐기며 이웃을 맞이하는 루틴을 시작함\\n* 이웃들이 지나갈 때 손을 흔들고, 인사하고, 이름을 적어두며 “기억에 남는 사람들”이 되기 위해 노력함\\n* 패티는 눈에 띄는 타이다이 모자도 착용해 친근한 이미지 강조'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aa57fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving json: 100%|██████████| 975/975 [00:00<00:00, 1599.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경과 시간: 0.618105411529541초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=len(filepaths), desc=\"Saving json\") as pbar:\n",
    "\n",
    "    info_id = 0\n",
    "    count = 0\n",
    "    for filepath in filepaths:\n",
    "        try:\n",
    "            info_id += 1\n",
    "        \n",
    "            filename = filepath.split(\"/\")[-1].strip()\n",
    "            \n",
    "            loader = TextLoader(filepath)\n",
    "\n",
    "            docs = loader.load_and_split(text_splitter)\n",
    "            chunks = [\n",
    "                [p.strip() for p in doc.page_content.split(\"\\n\") if p.strip()]\n",
    "                for doc in docs\n",
    "            ]\n",
    "            \n",
    "            params = Params(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                length_function=\"len\",\n",
    "                separator=SEPARATOR,\n",
    "                splitter=\"RecursiveCharacterTextSplitter\",\n",
    "                parser=\"TextLoader\"\n",
    "            )\n",
    "            \n",
    "            metadata = Metadata(\n",
    "                title=filename.split(\".md\")[0]\n",
    "            )\n",
    "\n",
    "            json_data = DataSpec(\n",
    "                ver=\"v1.2\",\n",
    "                access_level=1,\n",
    "                params=params,\n",
    "                metadata=metadata,\n",
    "                data=Data(\n",
    "                    chunks=chunks,\n",
    "                    section=filename.split(\".md\")[0]\n",
    "                )\n",
    "            ).model_dump()\n",
    "\n",
    "            save_json(filepath, filename, json_data)  \n",
    "            count += 1\n",
    "            logging.info(f\"File: {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(e, exc_info=True) \n",
    "        \n",
    "        pbar.update(1)             \n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"경과 시간: {elapsed_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918f50d",
   "metadata": {},
   "source": [
    "json2vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f15726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embedding_model():\n",
    "\n",
    "    return SentenceTransformer(\n",
    "        'Qwen/Qwen3-Embedding-0.6B',\n",
    "        device=\"cuda:0\",\n",
    "        model_kwargs={\"torch_dtype\": \"bfloat16\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ea5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def insert_data(\n",
    "            collection_name: str, \n",
    "            doc_batch: List[str]\n",
    "    ) -> None:\n",
    "    \n",
    "    # 임베딩 스펙 변경시 모델 재 로드\n",
    "    embedding_model = get_embedding_model()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    docs_embeddings = embedding_model.encode(doc_batch.page_content)\n",
    "    docs_embeddings_result = [docs_embeddings[i].tolist() for i in range(len(docs_embeddings))]\n",
    "\n",
    "    docs_embeddings_result = {\"dense\": docs_embeddings_result}\n",
    "\n",
    "    docs_embeddings = {}\n",
    "    dense_vector = docs_embeddings_result[\"dense\"]\n",
    "\n",
    "    docs_embeddings[\"dense\"] = dense_vector\n",
    "    rows = len(docs_embeddings[\"dense\"])\n",
    "\n",
    "    if \"sparse\" in docs_embeddings_result.keys():\n",
    "        sparse_vector = docs_embeddings_result[\"sparse\"]\n",
    "        docs_embeddings[\"sparse\"] = csr_matrix(\n",
    "            (sparse_vector[\"data\"], sparse_vector[\"indices\"], sparse_vector[\"indptr\"]),\n",
    "            shape=tuple(sparse_vector[\"shape\"])\n",
    "        )\n",
    "\n",
    "    for idx in range(rows):\n",
    "\n",
    "        dense_vector = docs_embeddings[\"dense\"][idx]\n",
    "\n",
    "        row = {\n",
    "            \"ver\": doc_batch.ver,\n",
    "            \"access_level\": doc_batch.access_level,\n",
    "            \"params\": doc_batch.params,\n",
    "            \"dense_vector\": dense_vector,\n",
    "            \"section\": doc_batch.section,\n",
    "            \"chunk\": doc_batch.page_content[idx],\n",
    "            \"metadata\": doc_batch.metadata\n",
    "        }\n",
    "\n",
    "        if \"sparse\" in docs_embeddings_result.keys():\n",
    "            row[\"sparse_vector\"] = sparse_vector = docs_embeddings[\"sparse\"][[idx], :]\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "    # milvus client 에서 sparse 연산 후 insert 그러나 0.02초 소요\n",
    "    # sparse 자체는 연산 시간 얼마 안걸림\n",
    "    milvus_client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b8a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트를 정제하는 함수입니다.\n",
    "\n",
    "    이 함수는 다음과 같은 작업을 수행합니다:\n",
    "    1. 유니코드 정규화\n",
    "    2. URL 제거\n",
    "    3. 특정 문자만 유지하고 나머지 제거\n",
    "    4. 빈 괄호 제거\n",
    "    5. 연속된 줄바꿈과 공백 정리\n",
    "    6. 중복된 문장부호 정리\n",
    "\n",
    "    Args:\n",
    "        text (str): 정제할 원본 텍스트\n",
    "\n",
    "    Returns:\n",
    "        str: 정제된 텍스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 유니코드 정규화 및 url 제거\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'(?:https?:\\/\\/)?(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text)\n",
    "    \n",
    "    # 한글, 영문, 숫자, 특정 문장부호만 남기고 나머지는 제거\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9.,!?\"\\'\\s\\-():;/=%{}[\\]<>]', '', text)\n",
    "\n",
    "    # 빈 괄호는 제거 (괄호 안에 공백 포함)\n",
    "    text = re.sub(r'\\(\\s*\\)|\\{\\s*\\}|\\[\\s*\\]|<\\s*>', '', text)\n",
    "\n",
    "    # 연속된 줄바꿈을 하나로 변환\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # 줄바꿈을 제외한 연속된 공백을 하나의 공백으로 변환\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    \n",
    "    # 중복된 문장부호를 하나로 줄임 (마침표, 쉼표, 느낌표, 물음표만)\n",
    "    text = re.sub(r'([.,!?])\\1+', r'\\1', text)\n",
    "    \n",
    "    # 문장의 양 끝에 있는 불필요한 공백 제거\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f9269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, pk=None, ver=None, access_level=None, file=None, params=None, metadata=None, page_content=None, section=None):\n",
    "        self.pk = pk\n",
    "        self.ver = ver\n",
    "        self.access_level = access_level\n",
    "        self.params = params\n",
    "        self.metadata = metadata\n",
    "        self.page_content = page_content\n",
    "        self.section = section\n",
    "\n",
    "# 동적 리턴이 필요한 컬렉션\n",
    "def get_collection(collection):\n",
    "    if collection == \"**\":\n",
    "        return os.path.basename(os.path.dirname(filepath))    \n",
    "    return collection\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "JSON_PATTERN = \"/home/woobin.choi/research/mlops/rag/rsc/processed/**/*.json\"\n",
    "json_files = list(glob.iglob(JSON_PATTERN, recursive=True))\n",
    "filepaths = [pdf for pdf in json_files ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8db1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "DATA_PATH = \"/home/woobin.choi/research/mlops/rag/rsc/processed\"\n",
    "\n",
    "with tqdm(total=len(filepaths), desc=\"Embedding/Insert\") as pbar:\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        with open(os.path.join(DATA_PATH, filepath), 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        metadata = json_data[\"metadata\"]\n",
    "        expr_title = metadata[\"title\"]\n",
    "        expr = f'metadata[\"title\"] == \\'{expr_title}\\''\n",
    "        json_title = metadata.get(\"title\")\n",
    "        category = metadata.get(\"categories\")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            chunks = json_data[\"data\"][\"chunks\"]\n",
    "            # page_content = []\n",
    "\n",
    "            if not chunks:\n",
    "                continue\n",
    "\n",
    "            # for chunk in chunks:\n",
    "                # page_content.append(clean_text(\"\\n\".join(chunk)))\n",
    "            BATCH_CHUNK_SIZE = 10\n",
    "\n",
    "            for i in range(0, len(chunks), BATCH_CHUNK_SIZE):\n",
    "                page_content = []\n",
    "                for chunk in chunks[i:i+BATCH_CHUNK_SIZE]:\n",
    "                    page_content.append(clean_text(\"\\n\".join(chunk)))\n",
    "\n",
    "                doc = Document(\n",
    "                    ver=json_data[\"ver\"],\n",
    "                    access_level=json_data[\"access_level\"],\n",
    "                    params=json_data[\"params\"],\n",
    "                    page_content=page_content, \n",
    "                    metadata=metadata,\n",
    "                    section=json_data[\"data\"][\"section\"]\n",
    "                )\n",
    "\n",
    "                insert_data(\n",
    "                    collection_name = collection_name,\n",
    "                    doc_batch=doc\n",
    "                )\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('### error: ', json_title)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"경과 시간: {elapsed_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a0d32",
   "metadata": {},
   "source": [
    "Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cccbefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"mcp 에 대해서 알려줘\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ca5de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def get_output_fields(\n",
    "    collection_name,\n",
    ") -> List[str]:\n",
    "    res = milvus_client.describe_collection(\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    return [\n",
    "        field['name'] \n",
    "        for field in res['fields']\n",
    "        if field['name'] not in ('sparse_vector', 'dense_vector')\n",
    "    ]        \n",
    "\n",
    "def rrf_rank(\n",
    "    results_list: List[List[Dict]], \n",
    "    top_k: int, \n",
    ") -> List[Dict]:\n",
    "    scores = {}\n",
    "    entities = {}\n",
    "\n",
    "    for result_set in results_list:\n",
    "        for rank, item in enumerate(result_set):\n",
    "            doc_id = item['pk']\n",
    "            score = 1 / (rank + 1)\n",
    "            scores[doc_id] = scores.get(doc_id, 0) + score\n",
    "\n",
    "            if doc_id not in entities:\n",
    "                entities[doc_id] = item\n",
    "\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [entities[doc_id] for doc_id, _ in sorted_docs[:top_k]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54caed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    client,\n",
    "    collection_name: str,        \n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    max_allowed_level: Optional[int] = 0,\n",
    "    search_params: Optional[Dict] = {\"metric_type\": \"IP\"},\n",
    ") -> List[Document]:\n",
    "\n",
    "    embeddings_model = get_embedding_model()\n",
    "    embeddings_result = embeddings_model.encode(query)\n",
    "    # dense_vector = [np.array(embedding) for embedding in embeddings_result[\"dense\"]]\n",
    "    \n",
    "    dense_arr = np.asarray(embeddings_result, dtype=np.float32)\n",
    "    if dense_arr.ndim == 1:  # (D,)면 1개 배치로 감싸기\n",
    "        dense_vector = [dense_arr]\n",
    "    else:  # (N, D)\n",
    "        dense_vector = [row for row in dense_arr]\n",
    "\n",
    "    embeddings_result = {\"dense\": dense_vector}\n",
    "    \n",
    "    output_fields = get_output_fields(collection_name)\n",
    "    if max_allowed_level == 0:\n",
    "        expr = None \n",
    "    else:\n",
    "        expr = f\"access_level < {max_allowed_level + 1}\"        \n",
    "\n",
    "    try:\n",
    "        if \"sparse\" in embeddings_result.keys():\n",
    "            sparse_vector = csr_matrix(\n",
    "                (\n",
    "                    embeddings_result[\"sparse\"][\"data\"],\n",
    "                    embeddings_result[\"sparse\"][\"indices\"],\n",
    "                    embeddings_result[\"sparse\"][\"indptr\"],\n",
    "                ),\n",
    "                shape=tuple(embeddings_result[\"sparse\"][\"shape\"]),\n",
    "            )\n",
    "            sparse_res = client.search(\n",
    "                collection_name=collection_name,\n",
    "                filter=expr,\n",
    "                data=sparse_vector,\n",
    "                anns_field=\"sparse_vector\",\n",
    "                search_params=search_params,\n",
    "                limit=top_k,\n",
    "                output_fields=output_fields,\n",
    "            )            \n",
    "        else:\n",
    "            sparse_res = client.search(\n",
    "                collection_name=collection_name,\n",
    "                filter=expr,\n",
    "                data=[query],\n",
    "                anns_field=\"sparse_vector\",\n",
    "                # search_params=search_params,\n",
    "                limit=top_k,\n",
    "                output_fields=output_fields,\n",
    "            )\n",
    "\n",
    "        dense_res = client.search(\n",
    "            collection_name=collection_name,\n",
    "            filter=expr,\n",
    "            data=dense_vector,\n",
    "            anns_field=\"dense_vector\",\n",
    "            search_params=search_params,\n",
    "            limit=top_k,\n",
    "            output_fields=output_fields,\n",
    "        )\n",
    "\n",
    "        res = rrf_rank([sparse_res[0], dense_res[0]], top_k=top_k)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "    return [Document(\n",
    "        pk=result[\"entity\"][\"pk\"] if \"pk\" in result[\"entity\"].keys() else None,\n",
    "        ver=result[\"entity\"][\"ver\"] if \"ver\" in result[\"entity\"].keys() else None,\n",
    "        access_level=result[\"entity\"][\"access_level\"] if \"access_level\" in result[\"entity\"].keys() else None,\n",
    "        params=result[\"entity\"][\"params\"] if \"params\" in result[\"entity\"].keys() else None,\n",
    "        section=result[\"entity\"][\"section\"] if \"section\" in result[\"entity\"].keys() else None,\n",
    "        page_content=result[\"entity\"][\"chunk\"] if \"chunk\" in result[\"entity\"].keys() else None,\n",
    "        metadata=result[\"entity\"][\"metadata\"] if \"metadata\" in result[\"entity\"].keys() else None,\n",
    "    ) for result in res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d26363d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범위 검색\n",
    "return_docs = hybrid_search(\n",
    "    client = milvus_client,\n",
    "    collection_name = collection_name,\n",
    "    query = query,\n",
    "    top_k = 5,\n",
    "    max_allowed_level = 1,\n",
    "    search_params={\n",
    "        \"metric_type\": \"IP\",\n",
    "        \"params\": {\n",
    "            \"radius\": 0.4,\n",
    "            \"range_filter\": 0.6,\n",
    "        }\n",
    "    }\n",
    ")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1c7040c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(return_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3331849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20109\n",
      "===============================================\n",
      "배포 및 DNS 설정\n",
      " Heroku 배포 자동화, 일부 구버전 API 사용 문제는 문서 링크로 해결\n",
      " GoDaddy 도메인 연결도 버튼 누를 위치와 값까지 알려줘 손쉽게 설정 완료\n",
      " AI 도구로서의 Windsurf 사용 경험\n",
      "===============================================\n",
      "20721\n",
      "===============================================\n",
      "WinDBG(CDB)를 파이썬으로 제어하며, 이를 AI가 사용할 수 있도록 MCP 프로토콜 서버로 래핑함\n",
      " MCP는 Anthropic이 개발한 AI와 외부 도구 간의 통신 표준으로, 툴을 AI의 손처럼 사용할 수 있게 해줌\n",
      " MCP의 장점:\n",
      " 모든 AI 모델에서 사용 가능\n",
      " VS Code 외 환경에서도 독립 실행 가능\n",
      " 비플랫폼 종속적\n",
      " 빠른 기능 확장성 확보\n",
      "===============================================\n",
      "19987\n",
      "===============================================\n",
      "Playwright MCP - LLM을 위한 웹 브라우저 자동화용 MCP 서버\n",
      "===============================================\n",
      "21155\n",
      "===============================================\n",
      "Model Context Protocol (MCP) 지원 예정\n",
      " MCP는 LLM이 툴에 접근하는 새로운 표준 프로토콜로 급부상\n",
      " 지난 8일 내에 OpenAI, Anthropic, Mistral 등 대형 벤더 API에도 빠르게 도입되고 있음\n",
      " 향후 LLM을 MCP 클라이언트로 만들어 다양한 MCP 서버에 쉽게 연동 계획\n",
      "===============================================\n",
      "20430\n",
      "===============================================\n",
      "Claude Code는 시스템을 변경할 수 있는 작업(파일 쓰기, bash 명령어 실행, MCP 도구 사용 등)에 대해 기본적으로 사용자 승인 요청을 함\n",
      "이는 보안을 위한 보수적 설계이며, 사용자가 안전하다고 판단되는 도구는 허용 목록(allowlist) 을 통해 사전 승인 가능함\n",
      " 허용 도구 설정 방법\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(return_docs)):\n",
    "    print(return_docs[i].section)\n",
    "    print(\"===============================================\")\n",
    "    print(return_docs[i].page_content)\n",
    "    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292979a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
