## Expand the name of the chart
nameOverride: ""

## Default fully qualified app name
fullnameOverride: ""

## Enable or disable Milvus Cluster mode
cluster:
  enabled: false # woobin.choi

image:
  all:
    repository: milvusdb/milvus
    tag: v2.5.9
    pullPolicy: IfNotPresent
    # pullSecrets:
    #   - myRegistryKeySecretName
  tools:
    repository: milvusdb/milvus-config-tool
    tag: v0.1.2
    pullPolicy: IfNotPresent

# Global node selector (모든 컴포넌트 기본 적용)
nodeSelector:
  system: vdb

# Global tolerations (모든 컴포넌트 기본 적용)
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "gpu1"
    effect: "NoSchedule"

# Global affinity
affinity: {}

# Global securityContext
securityContext: {}
  # runAsUser: 1000
  # runAsGroup: 1000
  # fsGroup: 1000
  # runAsNonRoot: true

# Global topologySpreadConstraints
topologySpreadConstraints: []
# - maxSkew: 1
#   topologyKey: topology.kubernetes.io/zone
#   whenUnsatisfiable: DoNotSchedule
#   labelSelector:
#     matchLabels:
#       app.kubernetes.io/name: milvus

# Global labels and annotations
labels: {}
annotations: {}

# Global Volumes & volumeMounts
volumes: []
volumeMounts: []

# Experimental feature: streaming node
streaming:
  enabled: false

customConfigMap: ""

extraConfigFiles:
  user.yaml: |+
    #    For example enable rest http for milvus proxy
    #    proxy:
    #      http:
    #        enabled: true
    #      maxUserNum: 100
    #      maxRoleNum: 10
    ##  Enable tlsMode and set the tls cert and key
    #  tls:
    #    serverPemPath: /etc/milvus/certs/tls.crt
    #    serverKeyPath: /etc/milvus/certs/tls.key
    #   common:
    #     security:
    #       tlsMode: 1

service:
  type: NodePort # woobin.choi
  port: 19530
  portName: milvus
  nodePort: ""
  annotations: {}
  labels: {}
  externalIPs: []
  loadBalancerSourceRanges:
  - 0.0.0.0/0
  # loadBalancerIP: 1.2.3.4

ingress:
  enabled: false
  ingressClassName: ""
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
    nginx.ingress.kubernetes.io/listen-ports-ssl: '[19530]'
    nginx.ingress.kubernetes.io/proxy-body-size: 4m
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  labels: {}
  rules:
    - host: "milvus-example.local"
      path: "/"
      pathType: "Prefix"
  tls: []

serviceAccount:
  create: false
  name:
  annotations:
  labels:

metrics:
  enabled: true
  serviceMonitor:
    enabled: false
    interval: "30s"
    scrapeTimeout: "10s"
    additionalLabels: {}

livenessProbe:
  enabled: true
  initialDelaySeconds: 90
  periodSeconds: 30
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 5

readinessProbe:
  enabled: true
  initialDelaySeconds: 90
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 5

log:
  level: "info"
  file:
    maxSize: 300
    maxAge: 10
    maxBackups: 20
  format: "text"
  persistence:
    mountPath: "/milvus/logs"
    enabled: false
    annotations:
      helm.sh/resource-policy: keep
    persistentVolumeClaim:
      existingClaim: ""
      storageClass:
      accessModes: ReadWriteMany
      size: 10Gi
      subPath: ""

heaptrack:
  image:
    repository: milvusdb/heaptrack
    tag: v0.1.0
    pullPolicy: IfNotPresent

standalone:
  replicas: 1
  priorityClassName: high-priority # woobin.choi
  resources: {}
  # 전역 nodeSelector/ tolerations 적용 + 컴포넌트에도 명시(가독성/안전)
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  disk:
    enabled: true
    size:
      enabled: false
  profiling:
    enabled: false
  messageQueue: rocksmq
  persistence:
    mountPath: "/var/lib/milvus"
    enabled: true
    annotations:
      helm.sh/resource-policy: keep
    persistentVolumeClaim:
      existingClaim: ""
      storageClass:
      accessModes: ReadWriteOnce
      size: 50Gi
      subPath: ""

proxy:
  enabled: true
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  http:
    enabled: true
    debugMode:
      enabled: false
  tls:
    enabled: false
  strategy: {}
  annotations: {}
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    cpuUtilization: 40

rootCoordinator:
  enabled: false
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  activeStandby:
    enabled: false
  strategy: {}
  annotations: {}
  service:
    port: 53100
    annotations: {}
    labels: {}
    clusterIP: ""

queryCoordinator:
  enabled: false
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  activeStandby:
    enabled: false
  strategy: {}
  annotations: {}
  service:
    port: 19531
    annotations: {}
    labels: {}
    clusterIP: ""

queryNode:
  enabled: true
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  disk:
    enabled: true
    size:
      enabled: false
  profiling:
    enabled: false
  strategy: {}
  annotations: {}
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    cpuUtilization: 40
  memoryUtilization: 60

indexCoordinator:
  enabled: false
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  activeStandby:
    enabled: false
  strategy: {}
  annotations: {}
  service:
    port: 31000
    annotations: {}
    labels: {}
    clusterIP: ""

indexNode:
  enabled: true
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  disk:
    enabled: true
    size:
      enabled: false
  strategy: {}
  annotations: {}
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    cpuUtilization: 40

dataCoordinator:
  enabled: false
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  activeStandby:
    enabled: false
  strategy: {}
  annotations: {}
  service:
    port: 13333
    annotations: {}
    labels: {}
    clusterIP: ""

dataNode:
  enabled: true
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  strategy: {}
  annotations: {}
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    cpuUtilization: 40

## mixCoordinator contains all coord
mixCoordinator:
  enabled: true
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  topologySpreadConstraints: []
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  activeStandby:
    enabled: false
  strategy: {}
  annotations: {}
  service:
    annotations: {}
    labels: {}
    clusterIP: ""

streamingNode:
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  securityContext: {}
  extraEnv: []
  heaptrack:
    enabled: false
  profiling:
    enabled: false
  strategy: {}

attu:
  enabled: true # woobin.choi
  name: attu
  image:
    repository: zilliz/attu
    tag: v2.5.3
    pullPolicy: IfNotPresent
  service:
    annotations: {}
    labels: {}
    type: NodePort # woobin.choi
    port: 3000
  resources: {}
  securityContext: {}
  podLabels: {}
  annotations: {}
  # attu도 gpu1에 고정하려면 tolerations 추가
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  ingress:
    enabled: false
    ingressClassName: ""
    annotations: {}
    labels: {}
    hosts:
      - milvus-attu.local
    tls: []

## Configuration values for the minio dependency
minio:
  enabled: true
  name: minio
  mode: standalone # woobin.choi default: distributed
  priorityClassName: high-priority # woobin.choi
  image:
    tag: "RELEASE.2023-03-20T20-16-18Z"
    pullPolicy: IfNotPresent
  accessKey: minioadmin
  secretKey: minioadmin
  existingSecret: ""
  bucketName: "milvus-bucket"
  rootPath: file
  useIAM: false
  iamEndpoint: ""
  region: ""
  useVirtualHost: false
  podDisruptionBudget:
    enabled: false
  resources:
    requests:
      memory: 2Gi
  nodeSelector:
    nodepool-name: gpu1
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  service:
    type: NodePort # woobin.choi
    port: 9000
  persistence:
    enabled: true
    existingClaim: ""
    storageClass:
    accessMode: ReadWriteOnce
    size: 500Gi
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 5
  startupProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 60

## Configuration values for the etcd dependency
etcd:
  enabled: true
  name: etcd
  replicaCount: 1 # woobin.choi
  priorityClassName: high-priority # woobin.choi
  pdb:
    create: false
  image:
    repository: "milvusdb/etcd"
    tag: "3.5.18-r1"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 2379
    peerPort: 2380
  nodeSelector:
    nodepool-name: gpu1
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "gpu1"
      effect: "NoSchedule"
  auth:
    rbac:
      enabled: false
  persistence:
    enabled: true
    storageClass:
    accessMode: ReadWriteOnce
    size: 10Gi
  livenessProbe:
    enabled: true
    timeoutSeconds: 10
  readinessProbe:
    enabled: true
    periodSeconds: 20
    timeoutSeconds: 10
  autoCompactionMode: revision
  autoCompactionRetention: "1000"
  extraEnvVars:
  - name: ETCD_QUOTA_BACKEND_BYTES
    value: "4294967296"
  - name: ETCD_HEARTBEAT_INTERVAL
    value: "500"
  - name: ETCD_ELECTION_TIMEOUT
    value: "2500"

## Pulsar/Kafka/External* (기존 유지)
pulsarv3:
  name: "pulsarv3"
  nameOverride: "pulsarv3"
  enabled: false # woobin.choi
  persistence: true
  extra: {}
  monitoring: {}
  volumes:
    persistence: true
    local_storage: false
  components:
    zookeeper: true
    bookkeeper: true
    autorecovery: true
    broker: true
    functions: false
    proxy: true
    toolset: false
    pulsar_manager: false
  images:
    zookeeper:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
    bookie:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
    autorecovery:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
    broker:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
    proxy:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
  zookeeper:
    pdb:
      usePolicy: false
    affinity:
      anti_affinity: false
    component: zookeeper
    replicaCount: 3
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    podMonitor:
      enabled: false
    resources:
      requests:
        memory: 256Mi
        cpu: 0.2
    volumes:
      persistence: true
      data:
        name: data
        size: 20Gi
        local_storage: false
    configData:
      PULSAR_MEM: >
        -Xms256m -Xmx256m
      PULSAR_GC: >
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=10
        -Dcom.sun.management.jmxremote
        -Djute.maxbuffer=10485760
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:+DisableExplicitGC
        -XX:+ExitOnOutOfMemoryError
        -XX:+PerfDisableSharedMem
  bookkeeper:
    pdb:
      usePolicy: false
    affinity:
      anti_affinity: false
    component: bookie
    replicaCount: 3
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    podMonitor:
      enabled: false
    resources:
      requests:
        memory: 2048Mi
        cpu: 0.5
    volumes:
      persistence: true
      journal:
        name: journal
        size: 100Gi
        local_storage: false
      ledgers:
        name: ledgers
        size: 200Gi
        local_storage: false
    configData:
      PULSAR_MEM: >
        -Xms4096m
        -Xmx4096m
        -XX:MaxDirectMemorySize=8192m
      PULSAR_GC: >
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=10
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:ParallelGCThreads=4
        -XX:ConcGCThreads=4
        -XX:G1NewSizePercent=50
        -XX:+DisableExplicitGC
        -XX:-ResizePLAB
        -XX:+ExitOnOutOfMemoryError
        -XX:+PerfDisableSharedMem
      nettyMaxFrameSizeBytes: "104867840"
  autorecovery:
    affinity:
      anti_affinity: false
    component: recovery
    replicaCount: 1
    resources:
      requests:
        memory: 128Mi
        cpu: 0.1
    podMonitor:
      enabled: false
    configData:
      BOOKIE_MEM: >
        -Xms128m -Xmx128m
      PULSAR_PREFIX_useV2WireProtocol: "true"
  pulsar_metadata:
    component: pulsar-init
    image:
      repository: apachepulsar/pulsar
      tag: 3.0.7
      pullPolicy: IfNotPresent
  broker:
    pdb:
      usePolicy: false
    affinity:
      anti_affinity: false
    component: broker
    replicaCount: 2
    autoscaling:
      enabled: false
    podMonitor:
      enabled: false
    resources:
      requests:
        memory: 2048Mi
        cpu: 0.5
    configData:
      PULSAR_MEM: >
        -Xms4096m -Xmx4096m -XX:MaxDirectMemorySize=8192m
      PULSAR_GC: >
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=10
        -Dio.netty.leakDetectionLevel=disabled
        -Dio.netty.recycler.linkCapacity=1024
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:ParallelGCThreads=4
        -XX:ConcGCThreads=4
        -XX:G1NewSizePercent=50
        -XX:+DisableExplicitGC
        -XX:-ResizePLAB
        -XX:+ExitOnOutOfMemoryError
        -XX:+PerfDisableSharedMem
      managedLedgerDefaultEnsembleSize: "3"
      managedLedgerDefaultWriteQuorum: "3"
      managedLedgerDefaultAckQuorum: "2"
      maxMessageSize: "104857600"
      defaultRetentionTimeInMinutes: "10080"
      defaultRetentionSizeInMB: "-1"
      backlogQuotaDefaultLimitGB: "8"
      ttlDurationDefaultInSeconds: "259200"
      subscriptionExpirationTimeMinutes: "3"
      backlogQuotaDefaultRetentionPolicy: producer_exception
  proxy:
    pdb:
      usePolicy: false
    affinity:
      anti_affinity: false
    component: proxy
    replicaCount: 2
    autoscaling:
      enabled: false
    podMonitor:
      enabled: false
    resources:
      requests:
        memory: 1024Mi
        cpu: 0.5
    configData:
      PULSAR_MEM: >
        -Xms512m -Xmx512m -XX:MaxDirectMemorySize=2048m
      PULSAR_GC: >
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=10
        -Dio.netty.leakDetectionLevel=disabled
        -Dio.netty.recycler.linkCapacity=1024
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:ParallelGCThreads=4
        -XX:ConcGCThreads=4
        -XX:G1NewSizePercent=50
        -XX:+DisableExplicitGC
        -XX:-ResizePLAB
        -XX:+ExitOnOutOfMemoryError
        -XX:+PerfDisableSharedMem
      httpNumThreads: "8"
    ports:
      http: 80
      https: 443
      pulsar: 6650
      pulsarssl: 6651
      containerPorts:
        http: 8080
        https: 8443
    service:
      annotations: {}
      type: ClusterIP
  kube-prometheus-stack:
    crds:
      enabled: false
    enabled: false
    prometheus:
      enabled: false
    grafana:
      enabled: false

pulsar:
  enabled: false
  name: pulsar
  fullnameOverride: ""
  persistence: true
  maxMessageSize: "5242880"
  rbac:
    enabled: false
    psp: false
    limit_to_namespace: true
  affinity:
    anti_affinity: false
  components:
    zookeeper: true
    bookkeeper: true
    autorecovery: true
    broker: true
    functions: false
    proxy: true
    toolset: false
    pulsar_manager: false
  monitoring:
    prometheus: false
    grafana: false
    node_exporter: false
    alert_manager: false
  images:
    broker:
      repository: apachepulsar/pulsar
      pullPolicy: IfNotPresent
      tag: 2.9.5
    autorecovery:
      repository: apachepulsar/pulsar
      tag: 2.9.5
      pullPolicy: IfNotPresent
    zookeeper:
      repository: apachepulsar/pulsar
      pullPolicy: IfNotPresent
      tag: 2.9.5
    bookie:
      repository: apachepulsar/pulsar
      pullPolicy: IfNotPresent
      tag: 2.9.5
    proxy:
      repository: apachepulsar/pulsar
      pullPolicy: IfNotPresent
      tag: 2.9.5
    pulsar_manager:
      repository: apachepulsar/pulsar-manager
      pullPolicy: IfNotPresent
      tag: v0.1.0
  zookeeper:
    resources:
      requests:
        memory: 1024Mi
        cpu: 0.3
    configData:
      PULSAR_MEM: >
        -Xms1024m
        -Xmx1024m
      PULSAR_GC: >
         -Dcom.sun.management.jmxremote
         -Djute.maxbuffer=10485760
         -XX:+ParallelRefProcEnabled
         -XX:+UnlockExperimentalVMOptions
         -XX:+DoEscapeAnalysis
         -XX:+DisableExplicitGC
         -XX:+PerfDisableSharedMem
         -Dzookeeper.forceSync=no
    pdb:
      usePolicy: false
  bookkeeper:
    replicaCount: 3
    volumes:
      journal:
        name: journal
        size: 100Gi
      ledgers:
        name: ledgers
        size: 200Gi
    resources:
      requests:
        memory: 2048Mi
        cpu: 1
    configData:
      PULSAR_MEM: >
        -Xms4096m
        -Xmx4096m
        -XX:MaxDirectMemorySize=8192m
      PULSAR_GC: >
        -Dio.netty.leakDetectionLevel=disabled
        -Dio.netty.recycler.linkCapacity=1024
        -XX:+UseG1GC -XX:MaxGCPauseMillis=10
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:ParallelGCThreads=32
        -XX:ConcGCThreads=32
        -XX:G1NewSizePercent=50
        -XX:+DisableExplicitGC
        -XX:-ResizePLAB
        -XX:+ExitOnOutOfMemoryError
        -XX:+PerfDisableSharedMem
        -XX:+PrintGCDetails
      nettyMaxFrameSizeBytes: "104867840"
    pdb:
      usePolicy: false
  broker:
    component: broker
    podMonitor:
      enabled: false
    replicaCount: 1
    resources:
      requests:
        memory: 4096Mi
        cpu: 1.5
    configData:
      PULSAR_MEM: >
        -Xms4096m
        -Xmx4096m
        -XX:MaxDirectMemorySize=8192m
      PULSAR_GC: >
        -Dio.netty.leakDetectionLevel=disabled
        -Dio.netty.recycler.linkCapacity=1024
        -XX:+ParallelRefProcEnabled
        -XX:+UnlockExperimentalVMOptions
        -XX:+DoEscapeAnalysis
        -XX:ParallelGCThreads=32
        -XX:ConcGCThreads=32
        -XX:G1NewSizePercent=50
        -XX:+DisableExplicitGC
        -XX:-ResizePLAB
        -XX:+ExitOnOutOfMemoryError
      maxMessageSize: "104857600"
      defaultRetentionTimeInMinutes: "10080"
      defaultRetentionSizeInMB: "-1"
      backlogQuotaDefaultLimitGB: "8"
      ttlDurationDefaultInSeconds: "259200"
      subscriptionExpirationTimeMinutes: "3"
      backlogQuotaDefaultRetentionPolicy: producer_exception
    pdb:
      usePolicy: false
  autorecovery:
    resources:
      requests:
        memory: 512Mi
        cpu: 1
  proxy:
    replicaCount: 1
    podMonitor:
      enabled: false
    resources:
      requests:
        memory: 2048Mi
        cpu: 1
    service:
      type: ClusterIP
    ports:
      pulsar: 6650
      http: 8080
    configData:
      PULSAR_MEM: >
        -Xms2048m -Xmx2048m
      PULSAR_GC: >
        -XX:MaxDirectMemorySize=2048m
      httpNumThreads: "100"
    pdb:
      usePolicy: false
  pulsar_manager:
    service:
      type: ClusterIP
  pulsar_metadata:
    component: pulsar-init
    image:
      repository: apachepulsar/pulsar
      tag: 2.9.5

kafka:
  enabled: false
  name: kafka
  replicaCount: 3
  image:
    repository: bitnami/kafka
    tag: 3.1.0-debian-10-r52
  terminationGracePeriodSeconds: "90"
  pdb:
    create: false
  startupProbe:
    enabled: true
  heapOpts: "-Xmx4096m -Xms4096m"
  maxMessageBytes: _10485760
  defaultReplicationFactor: 3
  offsetsTopicReplicationFactor: 3
  logRetentionHours: 168
  logRetentionBytes: _-1
  extraEnvVars:
  - name: KAFKA_CFG_MAX_PARTITION_FETCH_BYTES
    value: "5242880"
  - name: KAFKA_CFG_MAX_REQUEST_SIZE
    value: "5242880"
  - name: KAFKA_CFG_REPLICA_FETCH_MAX_BYTES
    value: "10485760"
  - name: KAFKA_CFG_FETCH_MESSAGE_MAX_BYTES
    value: "5242880"
  - name: KAFKA_CFG_LOG_ROLL_HOURS
    value: "24"
  persistence:
    enabled: true
    storageClass:
    accessMode: ReadWriteOnce
    size: 300Gi
  metrics:
    kafka:
      enabled: false
      image:
        repository: bitnami/kafka-exporter
        tag: 1.4.2-debian-10-r182
    jmx:
      enabled: false
      image:
        repository: bitnami/jmx-exporter
        tag: 0.16.1-debian-10-r245
    serviceMonitor:
      enabled: false
  service:
    type: ClusterIP
    ports:
      client: 9092
  zookeeper:
    enabled: true
    replicaCount: 3

woodpecker:
  enabled: false

externalS3:
  enabled: false
  host: ""
  port: ""
  accessKey: ""
  secretKey: ""
  useSSL: false
  bucketName: ""
  rootPath: ""
  useIAM: false
  cloudProvider: "aws"
  iamEndpoint: ""
  region: ""
  useVirtualHost: false

externalGcs:
  bucketName: ""

externalEtcd:
  enabled: false
  endpoints:
    - localhost:2379

externalPulsar:
  enabled: false
  host: localhost
  port: 6650
  maxMessageSize: "5242880"
  tenant: public
  namespace: poc-kanana-rag
  authPlugin: ""
  authParams: ""

externalKafka:
  enabled: false
  brokerList: localhost:9092
  securityProtocol: SASL_SSL
  sasl:
    mechanisms: PLAIN
    username: ""
    password: ""

tei:
  enabled: false
  name: text-embeddings-inference
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-1.6
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8080
    annotations: {}
    labels: {}
  resources:
    requests:
      cpu: "4"
      memory: "8Gi"
    limits:
      cpu: "8"
      memory: "16Gi"
  persistence:
    enabled: true
    mountPath: "/data"
    annotations: {}
    persistentVolumeClaim:
      existingClaim: ""
      storageClass:
      accessModes: ReadWriteOnce
      size: 50Gi
      subPath: ""
  modelId: "Qwen/Qwen3-Embedding-0.6B"
  extraArgs: []
  nodeSelector: {}
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
  extraEnv: []
